{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Classification for WiFi Localization via Keras/Tensorflow\n",
    "This notebook shows classification results of sections instead of zones. The classifier used for this task is the same as introduced in section '20180831_model_tuning/improved_classifier'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Python packages intalled\n",
    "    - pandas\n",
    "    - numpy\n",
    "    - tensorflow\n",
    "    - keras\n",
    "    - scikit-learn\n",
    "- Training data file 'fingerprints_gt_ver3_unpacked.csv' containing labelled data of WiFi fingerprints. (unpack it in the subdirectory '../data', if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(335663, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fr_observation_time</th>\n",
       "      <th>fr_values</th>\n",
       "      <th>fr_mac_address_id</th>\n",
       "      <th>zo_name</th>\n",
       "      <th>se_name</th>\n",
       "      <th>prev_fp_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73426</td>\n",
       "      <td>2015-12-08 13:47:17</td>\n",
       "      <td>{'236': '-81', '237': '-74'}</td>\n",
       "      <td>2002427</td>\n",
       "      <td>346</td>\n",
       "      <td>Anson's EG</td>\n",
       "      <td>73421.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>342941</td>\n",
       "      <td>2015-12-10 12:54:18</td>\n",
       "      <td>{'59': '-81', '60': '-68', '62': '-78', '63': ...</td>\n",
       "      <td>2002427</td>\n",
       "      <td>507</td>\n",
       "      <td>Peter Polzer</td>\n",
       "      <td>342937.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>187205</td>\n",
       "      <td>2015-12-09 10:20:32</td>\n",
       "      <td>{'74': '-70', '75': '-71', '76': '-77', '77': ...</td>\n",
       "      <td>2002427</td>\n",
       "      <td>303</td>\n",
       "      <td>Tesla</td>\n",
       "      <td>187198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215433</td>\n",
       "      <td>2015-12-09 12:07:59</td>\n",
       "      <td>{'58': '-72', '65': '-66', '67': '-68', '172':...</td>\n",
       "      <td>2002427</td>\n",
       "      <td>262</td>\n",
       "      <td>ZARA EG</td>\n",
       "      <td>215427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4397</td>\n",
       "      <td>2015-12-08 10:15:45</td>\n",
       "      <td>{'14': '-76', '16': '-79', '32': '-67', '33': ...</td>\n",
       "      <td>2002427</td>\n",
       "      <td>226</td>\n",
       "      <td>Vero Moda</td>\n",
       "      <td>4386.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  fr_observation_time  \\\n",
       "0       73426  2015-12-08 13:47:17   \n",
       "1      342941  2015-12-10 12:54:18   \n",
       "2      187205  2015-12-09 10:20:32   \n",
       "3      215433  2015-12-09 12:07:59   \n",
       "4        4397  2015-12-08 10:15:45   \n",
       "\n",
       "                                           fr_values  fr_mac_address_id  \\\n",
       "0                       {'236': '-81', '237': '-74'}            2002427   \n",
       "1  {'59': '-81', '60': '-68', '62': '-78', '63': ...            2002427   \n",
       "2  {'74': '-70', '75': '-71', '76': '-77', '77': ...            2002427   \n",
       "3  {'58': '-72', '65': '-66', '67': '-68', '172':...            2002427   \n",
       "4  {'14': '-76', '16': '-79', '32': '-67', '33': ...            2002427   \n",
       "\n",
       "   zo_name       se_name  prev_fp_idx  \n",
       "0      346    Anson's EG      73421.0  \n",
       "1      507  Peter Polzer     342937.0  \n",
       "2      303         Tesla     187198.0  \n",
       "3      262       ZARA EG     215427.0  \n",
       "4      226     Vero Moda       4386.0  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('~/data_sections/fingerprints_train_rev5.csv')\n",
    "df = df.append(pd.read_csv('~/data_sections/fingerprints_test_rev5.csv'))\n",
    "\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert section name to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(335663,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      2\n",
       "1    177\n",
       "2    212\n",
       "3    236\n",
       "4    226\n",
       "dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_train['se_name'])\n",
    "classes = pd.Series(le.transform(df['se_name']))\n",
    "print(classes.shape)\n",
    "classes.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack features (RSSI dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>107</th>\n",
       "      <th>109</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>89</th>\n",
       "      <th>9</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-74</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-68</td>\n",
       "      <td>-77</td>\n",
       "      <td>-83</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1    10   101   102   103   104   105   107   109    11  ...     89  \\\n",
       "0  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  ...   -100   \n",
       "1  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  ...   -100   \n",
       "2  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  ...   -100   \n",
       "3  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  ...   -100   \n",
       "\n",
       "      9    90    91    92    93    94    97    98    99  \n",
       "0  -100  -100  -100  -100  -100  -100  -100  -100  -100  \n",
       "1  -100  -100  -100  -100   -74  -100  -100  -100  -100  \n",
       "2  -100  -100   -68   -77   -83  -100  -100  -100  -100  \n",
       "3  -100  -100  -100  -100  -100  -100  -100  -100  -100  \n",
       "\n",
       "[4 rows x 261 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df['fr_values'].apply(lambda x : dict(eval(x)))\n",
    "features = features.apply(pd.Series)\n",
    "features.fillna(-100, inplace=True)\n",
    "features.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data\n",
    "Normilize features and split data into training, evaluation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (268530, 261) (268530, 1)\n",
      "Eval data shape: (33567, 261) (33567, 1)\n",
      "Test data shape: (33566, 261) (33566, 1)\n"
     ]
    }
   ],
   "source": [
    "X = features.astype('float32')\n",
    "Y = classes.astype('int')\n",
    "\n",
    "std_scale = preprocessing.MinMaxScaler().fit(X)\n",
    "X = std_scale.transform(X)\n",
    "\n",
    "# split data for training\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X, Y, stratify=Y, train_size=0.8, test_size=0.2)\n",
    "y_train = y_train.to_frame()\n",
    "y_rest = y_rest.to_frame()\n",
    "\n",
    "# split rest data into eval and test sets\n",
    "X_test, X_eval, y_test, y_eval = train_test_split(X_rest, y_rest, train_size=0.5, test_size=0.5)\n",
    "\n",
    "print('Training data shape:', X_train.shape, y_train.shape)\n",
    "print('Eval data shape:', X_eval.shape, y_eval.shape)\n",
    "print('Test data shape:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate one-hot matrices of target zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:3: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((268530, 240), (33567, 240), (33566, 240))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert 1-dimensional class arrays to one-hot class matrices\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "enc.fit(Y.reshape(-1, 1))\n",
    "\n",
    "Y_train = enc.transform(y_train.values.reshape(-1,1))\n",
    "Y_eval = enc.transform(y_eval.values.reshape(-1,1))\n",
    "Y_test = enc.transform(y_test.values.reshape(-1,1))\n",
    "    \n",
    "Y_train.shape, Y_eval.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 268530 samples, validate on 33567 samples\n",
      "Epoch 1/30\n",
      " - 72s - loss: 1.1052 - categorical_accuracy: 0.6454 - val_loss: 0.6487 - val_categorical_accuracy: 0.7705\n",
      "Epoch 2/30\n",
      " - 71s - loss: 0.6360 - categorical_accuracy: 0.7795 - val_loss: 0.5352 - val_categorical_accuracy: 0.8134\n",
      "Epoch 3/30\n",
      " - 71s - loss: 0.5324 - categorical_accuracy: 0.8138 - val_loss: 0.4755 - val_categorical_accuracy: 0.8334\n",
      "Epoch 4/30\n",
      " - 70s - loss: 0.4700 - categorical_accuracy: 0.8346 - val_loss: 0.4462 - val_categorical_accuracy: 0.8428\n",
      "Epoch 5/30\n",
      " - 71s - loss: 0.4258 - categorical_accuracy: 0.8506 - val_loss: 0.4214 - val_categorical_accuracy: 0.8538\n",
      "Epoch 6/30\n",
      " - 71s - loss: 0.3919 - categorical_accuracy: 0.8615 - val_loss: 0.4111 - val_categorical_accuracy: 0.8573\n",
      "Epoch 7/30\n",
      " - 71s - loss: 0.3644 - categorical_accuracy: 0.8709 - val_loss: 0.4003 - val_categorical_accuracy: 0.8616\n",
      "Epoch 8/30\n",
      " - 71s - loss: 0.3413 - categorical_accuracy: 0.8778 - val_loss: 0.3937 - val_categorical_accuracy: 0.8639\n",
      "Epoch 9/30\n",
      " - 71s - loss: 0.3226 - categorical_accuracy: 0.8841 - val_loss: 0.3907 - val_categorical_accuracy: 0.8665\n",
      "Epoch 10/30\n",
      " - 71s - loss: 0.3057 - categorical_accuracy: 0.8896 - val_loss: 0.3847 - val_categorical_accuracy: 0.8691\n",
      "Epoch 11/30\n",
      " - 70s - loss: 0.2894 - categorical_accuracy: 0.8949 - val_loss: 0.3872 - val_categorical_accuracy: 0.8682\n",
      "Epoch 12/30\n",
      " - 71s - loss: 0.2779 - categorical_accuracy: 0.8990 - val_loss: 0.3870 - val_categorical_accuracy: 0.8694\n",
      "Epoch 13/30\n",
      " - 71s - loss: 0.2663 - categorical_accuracy: 0.9035 - val_loss: 0.3828 - val_categorical_accuracy: 0.8716\n",
      "Epoch 14/30\n",
      " - 71s - loss: 0.2560 - categorical_accuracy: 0.9071 - val_loss: 0.3858 - val_categorical_accuracy: 0.8710\n",
      "Epoch 15/30\n",
      " - 71s - loss: 0.2482 - categorical_accuracy: 0.9096 - val_loss: 0.3837 - val_categorical_accuracy: 0.8715\n",
      "Epoch 16/30\n",
      " - 70s - loss: 0.2403 - categorical_accuracy: 0.9130 - val_loss: 0.3855 - val_categorical_accuracy: 0.8722\n",
      "Epoch 17/30\n",
      " - 71s - loss: 0.2318 - categorical_accuracy: 0.9155 - val_loss: 0.3892 - val_categorical_accuracy: 0.8732\n",
      "Epoch 18/30\n",
      " - 71s - loss: 0.2248 - categorical_accuracy: 0.9184 - val_loss: 0.3885 - val_categorical_accuracy: 0.8740\n",
      "Epoch 19/30\n",
      " - 71s - loss: 0.2193 - categorical_accuracy: 0.9199 - val_loss: 0.3866 - val_categorical_accuracy: 0.8747\n",
      "Epoch 20/30\n",
      " - 71s - loss: 0.2140 - categorical_accuracy: 0.9217 - val_loss: 0.3904 - val_categorical_accuracy: 0.8733\n",
      "Epoch 21/30\n",
      " - 48s - loss: 0.2074 - categorical_accuracy: 0.9241 - val_loss: 0.3930 - val_categorical_accuracy: 0.8745\n",
      "Epoch 22/30\n",
      " - 40s - loss: 0.2041 - categorical_accuracy: 0.9257 - val_loss: 0.3914 - val_categorical_accuracy: 0.8750\n",
      "Epoch 23/30\n",
      " - 40s - loss: 0.1992 - categorical_accuracy: 0.9268 - val_loss: 0.3917 - val_categorical_accuracy: 0.8756\n",
      "Epoch 24/30\n",
      " - 40s - loss: 0.1949 - categorical_accuracy: 0.9284 - val_loss: 0.3929 - val_categorical_accuracy: 0.8750\n",
      "Epoch 25/30\n",
      " - 40s - loss: 0.1908 - categorical_accuracy: 0.9299 - val_loss: 0.3985 - val_categorical_accuracy: 0.8748\n",
      "Epoch 26/30\n",
      " - 40s - loss: 0.1864 - categorical_accuracy: 0.9316 - val_loss: 0.3982 - val_categorical_accuracy: 0.8742\n",
      "Epoch 27/30\n",
      " - 40s - loss: 0.1827 - categorical_accuracy: 0.9331 - val_loss: 0.4003 - val_categorical_accuracy: 0.8739\n",
      "Epoch 28/30\n",
      " - 40s - loss: 0.1792 - categorical_accuracy: 0.9341 - val_loss: 0.4062 - val_categorical_accuracy: 0.8754\n",
      "Epoch 29/30\n",
      " - 40s - loss: 0.1769 - categorical_accuracy: 0.9355 - val_loss: 0.4053 - val_categorical_accuracy: 0.8744\n",
      "Epoch 30/30\n",
      " - 40s - loss: 0.1738 - categorical_accuracy: 0.9357 - val_loss: 0.4054 - val_categorical_accuracy: 0.8748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feedb246be0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, input_dim=261, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1000, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(500, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y_train.shape[1], kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "adamOpt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001, amsgrad=False)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adamOpt, metrics=['categorical_accuracy'])\n",
    "model.fit(X_train, Y_train, validation_data=(X_eval, Y_eval), batch_size=512, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "### Accuracy 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'categorical_accuracy']\n",
      "33566/33566 [==============================] - 3s 92us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.39417095220160697, 0.8805040814932954]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.metrics_names)\n",
    "model.evaluate(X_test, Y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score 0.88\n",
    "This relatively high F1 score of 0.88 is achieved also with good precision of 0.88.\n",
    "\n",
    "Note: A significant reduction of the score for classes with little supported observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.87      0.89       102\n",
      "          1       0.86      0.87      0.87       111\n",
      "          2       0.90      0.94      0.92       256\n",
      "          3       0.95      0.88      0.92       135\n",
      "          4       0.94      0.89      0.92        75\n",
      "          5       0.85      0.89      0.87        99\n",
      "          6       0.90      0.93      0.92       173\n",
      "          7       0.90      0.93      0.91       139\n",
      "          8       0.82      0.85      0.83       112\n",
      "          9       0.95      0.85      0.90       103\n",
      "         10       0.82      0.84      0.83        67\n",
      "         11       0.80      0.76      0.78        59\n",
      "         12       0.95      0.85      0.90       124\n",
      "         13       0.84      0.81      0.82        57\n",
      "         14       0.94      0.92      0.93        37\n",
      "         15       0.81      0.60      0.69       106\n",
      "         16       0.89      0.91      0.90       105\n",
      "         17       0.82      0.70      0.75        70\n",
      "         18       0.84      0.93      0.89        91\n",
      "         19       0.85      0.88      0.87       113\n",
      "         20       0.97      0.95      0.96        63\n",
      "         21       0.78      0.80      0.79        49\n",
      "         22       0.92      0.93      0.92        71\n",
      "         23       0.94      0.87      0.90       108\n",
      "         24       0.95      0.95      0.95       221\n",
      "         25       0.82      0.85      0.83        71\n",
      "         26       0.95      0.93      0.94        80\n",
      "         27       0.82      0.86      0.84        86\n",
      "         29       0.93      0.93      0.93        80\n",
      "         30       0.92      0.79      0.85        76\n",
      "         31       0.80      0.78      0.79        55\n",
      "         32       0.86      0.94      0.90        51\n",
      "         33       0.89      0.90      0.89        80\n",
      "         34       0.89      0.88      0.88        88\n",
      "         35       0.86      0.89      0.88       110\n",
      "         36       0.93      0.93      0.93       111\n",
      "         37       0.98      0.98      0.98       110\n",
      "         38       0.83      0.84      0.83       110\n",
      "         39       0.95      0.97      0.96       101\n",
      "         40       0.82      0.93      0.87        54\n",
      "         41       0.81      0.86      0.83        58\n",
      "         42       0.96      0.94      0.95        94\n",
      "         43       0.94      0.94      0.94        68\n",
      "         44       0.89      0.90      0.89        88\n",
      "         45       0.93      0.93      0.93       112\n",
      "         46       0.77      0.81      0.79        63\n",
      "         47       0.69      0.77      0.73        97\n",
      "         48       0.90      0.94      0.92       555\n",
      "         49       0.91      0.90      0.90       476\n",
      "         50       0.90      0.91      0.90       728\n",
      "         51       0.86      0.88      0.87       412\n",
      "         52       0.92      0.88      0.90       649\n",
      "         53       0.87      0.84      0.85       525\n",
      "         54       0.84      0.83      0.83       300\n",
      "         55       0.85      0.87      0.86       469\n",
      "         56       0.92      0.91      0.92      1271\n",
      "         57       0.90      0.91      0.90       792\n",
      "         58       0.89      0.91      0.90       906\n",
      "         59       0.88      0.89      0.89       769\n",
      "         60       0.90      0.91      0.90       794\n",
      "         61       0.88      0.91      0.89       791\n",
      "         62       0.90      0.90      0.90      1030\n",
      "         63       0.92      0.89      0.91       596\n",
      "         64       0.87      0.89      0.88       383\n",
      "         65       0.91      0.89      0.90       906\n",
      "         66       0.94      0.91      0.93       613\n",
      "         67       0.88      0.91      0.89       816\n",
      "         68       0.89      0.84      0.86        95\n",
      "         69       0.91      0.88      0.89        57\n",
      "         70       0.95      0.88      0.91        96\n",
      "         71       0.89      0.94      0.92        71\n",
      "         72       0.93      0.90      0.92       133\n",
      "         73       0.97      0.94      0.96        82\n",
      "         74       0.86      0.93      0.89       218\n",
      "         75       0.82      0.74      0.78        42\n",
      "         76       0.86      0.79      0.83        63\n",
      "         77       0.88      0.85      0.86        86\n",
      "         78       0.90      0.87      0.88       100\n",
      "         79       0.88      0.83      0.86        36\n",
      "         80       0.92      0.90      0.91        80\n",
      "         81       0.89      0.93      0.91        88\n",
      "         82       0.00      0.00      0.00         8\n",
      "         83       0.81      0.71      0.76        86\n",
      "         84       0.87      0.89      0.88        80\n",
      "         85       0.74      0.80      0.77        75\n",
      "         86       0.93      0.80      0.86        65\n",
      "         87       0.92      0.87      0.89        91\n",
      "         88       0.92      0.84      0.88        83\n",
      "         89       0.73      0.83      0.78        63\n",
      "         90       0.88      0.87      0.88        86\n",
      "         91       0.78      0.88      0.83        49\n",
      "         92       0.95      0.71      0.81        85\n",
      "         93       0.90      0.91      0.91       121\n",
      "         94       0.80      0.85      0.82        46\n",
      "         95       0.89      0.95      0.92       121\n",
      "         96       0.77      0.87      0.82        69\n",
      "         97       0.86      0.84      0.85        93\n",
      "         98       0.94      0.93      0.93       185\n",
      "         99       0.87      0.93      0.90       259\n",
      "        100       0.88      0.92      0.90       102\n",
      "        101       0.90      0.83      0.87       120\n",
      "        102       0.63      0.59      0.61        41\n",
      "        103       0.75      0.91      0.82        54\n",
      "        104       0.77      0.74      0.76       100\n",
      "        105       0.85      0.91      0.88        32\n",
      "        106       0.92      0.88      0.90        98\n",
      "        107       0.76      0.73      0.74        22\n",
      "        108       0.85      0.85      0.85        52\n",
      "        109       0.84      0.92      0.88       100\n",
      "        110       0.93      0.90      0.91       315\n",
      "        111       0.92      0.80      0.86       101\n",
      "        112       0.79      0.76      0.78        76\n",
      "        113       0.79      0.89      0.84        63\n",
      "        114       0.75      0.78      0.76        83\n",
      "        115       0.85      0.87      0.86       108\n",
      "        116       0.89      0.91      0.90        55\n",
      "        117       0.52      0.58      0.55        77\n",
      "        118       0.75      0.75      0.75        40\n",
      "        119       0.77      0.94      0.84       189\n",
      "        120       0.83      0.77      0.80        57\n",
      "        121       0.82      0.88      0.85        48\n",
      "        122       0.88      0.84      0.85        67\n",
      "        123       0.88      0.95      0.92       104\n",
      "        124       0.84      0.84      0.84        79\n",
      "        125       0.89      0.84      0.87        77\n",
      "        126       0.92      0.80      0.86        45\n",
      "        127       0.75      0.76      0.76        84\n",
      "        128       0.92      0.91      0.92        80\n",
      "        129       0.89      0.92      0.91        74\n",
      "        130       0.94      0.95      0.95        84\n",
      "        131       0.75      0.70      0.72        96\n",
      "        132       0.79      0.77      0.78        98\n",
      "        133       0.84      0.91      0.87        64\n",
      "        134       0.87      0.80      0.83        50\n",
      "        135       0.97      0.97      0.97        67\n",
      "        136       0.91      0.95      0.93       100\n",
      "        137       0.69      0.85      0.76        91\n",
      "        138       0.84      0.73      0.78        86\n",
      "        139       0.63      0.68      0.66        63\n",
      "        140       0.83      0.91      0.87        65\n",
      "        141       0.93      0.80      0.86        35\n",
      "        142       0.95      0.90      0.93        92\n",
      "        143       0.90      0.90      0.90        69\n",
      "        144       0.73      0.72      0.73        46\n",
      "        145       0.79      0.92      0.85        87\n",
      "        146       0.71      0.81      0.76        63\n",
      "        147       0.71      0.75      0.73        87\n",
      "        148       0.94      0.93      0.93       669\n",
      "        149       0.74      0.71      0.73        84\n",
      "        150       0.84      0.93      0.88        82\n",
      "        151       0.97      0.92      0.95        79\n",
      "        152       0.92      0.78      0.85       106\n",
      "        153       0.69      0.82      0.75        11\n",
      "        154       0.90      0.90      0.90        84\n",
      "        155       0.64      0.78      0.71        74\n",
      "        156       0.76      0.74      0.75       105\n",
      "        157       0.95      0.95      0.95       111\n",
      "        158       0.75      0.86      0.80        79\n",
      "        159       0.83      0.84      0.84        83\n",
      "        160       0.74      0.69      0.71        54\n",
      "        161       0.91      0.91      0.91        94\n",
      "        162       0.80      0.83      0.81        86\n",
      "        163       0.77      0.80      0.79        55\n",
      "        164       0.85      0.81      0.83       167\n",
      "        165       0.90      0.87      0.88       109\n",
      "        166       0.79      0.80      0.80       100\n",
      "        167       0.85      0.76      0.80        62\n",
      "        168       0.78      0.91      0.84        46\n",
      "        169       0.85      0.96      0.90        53\n",
      "        170       0.98      1.00      0.99       123\n",
      "        171       1.00      0.85      0.92        72\n",
      "        172       0.93      0.96      0.95       108\n",
      "        173       0.97      0.93      0.95       124\n",
      "        174       0.96      0.88      0.92       132\n",
      "        175       0.87      0.89      0.88       104\n",
      "        176       0.96      0.91      0.94       145\n",
      "        177       0.88      0.80      0.83       108\n",
      "        178       0.85      0.88      0.86        89\n",
      "        179       0.94      0.92      0.93        64\n",
      "        180       0.92      0.98      0.95        87\n",
      "        181       0.87      0.89      0.88       172\n",
      "        182       0.91      0.88      0.90        98\n",
      "        183       0.84      0.84      0.84        58\n",
      "        184       0.95      0.85      0.90       123\n",
      "        185       0.96      0.89      0.92       120\n",
      "        186       0.91      0.85      0.88        97\n",
      "        187       0.77      0.72      0.74        67\n",
      "        188       0.76      0.85      0.81        75\n",
      "        189       0.86      0.86      0.86       125\n",
      "        190       0.85      0.81      0.83       119\n",
      "        191       0.75      0.66      0.70        58\n",
      "        192       0.96      0.85      0.90        78\n",
      "        193       0.93      0.97      0.94       115\n",
      "        194       0.78      0.83      0.81        48\n",
      "        195       0.82      0.81      0.82        69\n",
      "        196       0.60      0.63      0.61        49\n",
      "        197       0.92      0.89      0.91        54\n",
      "        198       0.90      0.88      0.89       117\n",
      "        199       0.85      0.87      0.86        70\n",
      "        200       0.73      0.80      0.76        10\n",
      "        201       0.88      0.92      0.90        65\n",
      "        202       0.65      0.53      0.58        66\n",
      "        203       0.89      0.82      0.85        66\n",
      "        204       0.89      0.92      0.90       113\n",
      "        205       0.78      0.85      0.81        41\n",
      "        206       0.93      0.81      0.86        98\n",
      "        207       0.97      0.93      0.95       119\n",
      "        208       0.85      0.79      0.82       100\n",
      "        209       0.91      0.82      0.87        51\n",
      "        210       0.93      0.97      0.95       114\n",
      "        211       0.92      0.85      0.89        95\n",
      "        212       0.84      0.87      0.86        99\n",
      "        213       0.98      0.94      0.96       102\n",
      "        214       0.80      0.77      0.79        53\n",
      "        215       0.93      0.89      0.91        90\n",
      "        216       0.93      0.86      0.90        66\n",
      "        217       0.90      0.87      0.88        69\n",
      "        218       0.82      0.78      0.80        65\n",
      "        219       0.88      0.93      0.90        61\n",
      "        220       0.87      0.92      0.90        89\n",
      "        221       0.87      0.86      0.87        86\n",
      "        222       0.82      0.88      0.85        72\n",
      "        223       0.84      0.99      0.91        84\n",
      "        224       0.91      0.91      0.91        79\n",
      "        225       0.89      0.84      0.86        85\n",
      "        226       0.89      0.85      0.87        82\n",
      "        227       0.89      0.83      0.86        65\n",
      "        228       0.87      0.93      0.90        72\n",
      "        229       0.94      0.87      0.91       118\n",
      "        230       0.87      0.86      0.86       119\n",
      "        231       0.85      0.90      0.88        63\n",
      "        232       0.87      0.88      0.87        95\n",
      "        233       0.93      0.88      0.91        49\n",
      "        234       0.84      0.87      0.85        61\n",
      "        235       0.87      0.95      0.91        64\n",
      "        236       0.85      0.87      0.86       131\n",
      "        237       0.87      0.81      0.84       102\n",
      "        238       0.75      0.84      0.79       111\n",
      "        239       0.92      0.87      0.89        67\n",
      "\n",
      "avg / total       0.88      0.88      0.88     33566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test, batch_size=32, verbose=2)\n",
    "Y_predicted = np.argmax(pred, axis=1)\n",
    "Y_truth = np.argmax(Y_test, axis=1)\n",
    "report = classification_report(Y_truth, Y_predicted)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
